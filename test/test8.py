import os  
import tempfile  
import streamlit as st  
from langchain_openai import ChatOpenAI, OpenAIEmbeddings  
from langchain.document_loaders import PyPDFLoader  
from langchain.memory import ConversationBufferMemory  
from langchain.memory.chat_message_histories import StreamlitChatMessageHistory  
from langchain.callbacks.base import BaseCallbackHandler  
from langchain.text_splitter import RecursiveCharacterTextSplitter  
from langchain_redis import RedisVectorStore
from redis import Redis as RedisClient
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.tools import Tool
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
import asyncio
from typing import Dict, Any, List

st.set_page_config(page_title="PDFæ–‡æ¡£é—®ç­”", page_icon="ğŸ¦œ")  
st.title("ğŸ¦œ PDFæ–‡æ¡£é—®ç­”")  
  
  
@st.cache_resource(ttl="1h")  
def configure_retriever(uploaded_files):  
    # Read documents  
    docs = []  
    temp_dir = tempfile.TemporaryDirectory()  
    for file in uploaded_files:  
        temp_filepath = os.path.join(temp_dir.name, file.name)  
        with open(temp_filepath, "wb") as f:  
            f.write(file.getvalue())  
        loader = PyPDFLoader(temp_filepath)  
        docs.extend(loader.load())  
  
    # Split documents  
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)  
    splits = text_splitter.split_documents(docs)  
  
    # Create embeddings and store in Redis vectordb
    embeddings = OpenAIEmbeddings()
    redis_url = "redis://localhost:6379/0"
    redis_client = RedisClient.from_url(redis_url, decode_responses=True)
    
    # ä½¿ç”¨å”¯ä¸€ç´¢å¼•åç§°
    index_name = "pdf_qa_index"
    
    # æ£€æŸ¥å¹¶æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§ç´¢å¼•
    keys = redis_client.keys(f"{index_name}:*")
    if keys:
        redis_client.delete(*keys)
    try:
        redis_client.execute_command(f"FT.DROPINDEX {index_name} DD")
    except:
        pass
    
    # åˆ›å»ºRediså‘é‡æ•°æ®åº“
    vectordb = RedisVectorStore.from_documents(
        documents=splits,
        embedding=embeddings,
        redis_url=redis_url,
        index_name=index_name
    )
  
    # Define retriever  
    retriever = vectordb.as_retriever(search_kwargs={"k": 2})  
  
    return retriever  
  
  
class StreamHandler(BaseCallbackHandler):  
    def __init__(self, container: st.delta_generator.DeltaGenerator, initial_text: str = ""):  
        self.container = container  
        self.text = initial_text  
        self.run_id_ignore_token = None  
  
    def on_llm_start(self, serialized: dict, prompts: list, **kwargs):  
        # Workaround to prevent showing the rephrased question as output  
        if prompts[0].startswith("Human"):  
            self.run_id_ignore_token = kwargs.get("run_id")  
  
    def on_llm_new_token(self, token: str, **kwargs) -> None:  
        if self.run_id_ignore_token == kwargs.get("run_id", False):  
            return  
        self.text += token  
        self.container.markdown(self.text)  
  
  
class PrintRetrievalHandler(BaseCallbackHandler):  
    def __init__(self, container):  
        self.status = container.status("**Context Retrieval**")  
  
    def on_retriever_start(self, serialized: dict, query: str, **kwargs):  
        self.status.write(f"**Question:** {query}")  
        self.status.update(label=f"**Context Retrieval:** {query}")  
  
    def on_retriever_end(self, documents, **kwargs):  
        for idx, doc in enumerate(documents):  
            source = os.path.basename(doc.metadata["source"])  
            self.status.write(f"**Document {idx} from {source}**")  
            self.status.markdown(doc.page_content)  
        self.status.update(state="complete")  
  
# åˆå§‹åŒ–session_state
if "response" not in st.session_state:
    st.session_state.response = None
if "msgs" not in st.session_state:
    st.session_state.msgs = StreamlitChatMessageHistory()

uploaded_files = st.sidebar.file_uploader(  
    label="ä¸Šä¼ PDFæ–‡ä»¶", type=["pdf"], accept_multiple_files=True  
)  
if not uploaded_files:  
    st.info("ä¸Šä¼ PDFæ–‡æ¡£åä½¿ç”¨")  
    st.stop()  
  
retriever = configure_retriever(uploaded_files)  
  
# Setup memory for contextual conversation  
msgs = st.session_state.msgs
memory = ConversationBufferMemory(memory_key="chat_history", chat_memory=msgs, return_messages=True)  
  
# åˆ›å»ºæ£€ç´¢å·¥å…·
async def retrieve_docs(query):
    docs = await retriever.ainvoke(query)
    return "\n\n".join([f"Document {i}: {doc.page_content}" for i, doc in enumerate(docs)])

retrieval_tool = Tool(
    name="pdf_search",
    description="æœç´¢PDFæ–‡æ¡£ä¸­çš„ä¿¡æ¯ï¼Œç”¨äºå›ç­”å…³äºæ–‡æ¡£å†…å®¹çš„é—®é¢˜",
    func=lambda query: asyncio.run(retrieve_docs(query))
)

# è®¾ç½®Agentæç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„PDFæ–‡æ¡£åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”ç”¨æˆ·å…³äºä¸Šä¼ æ–‡æ¡£çš„é—®é¢˜ã€‚ä½¿ç”¨æä¾›çš„å·¥å…·æ¥æœç´¢æ–‡æ¡£å†…å®¹ï¼Œå¹¶ç»™å‡ºè¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

# è®¾ç½®LLM
llm = ChatOpenAI(temperature=0, streaming=True)

# åˆ›å»ºAgent
agent = create_openai_tools_agent(llm, [retrieval_tool], prompt)
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=[retrieval_tool],
    memory=memory,
    verbose=True,
    return_intermediate_steps=True
)
  
if len(msgs.messages) == 0 or st.sidebar.button("Clear message history"):  
    msgs.clear()  
    msgs.add_ai_message("æˆ‘å¯ä»¥å¸®ä½ å›ç­”å…³äºä¸Šä¼ PDFæ–‡æ¡£çš„é—®é¢˜ã€‚")  
  
avatars = {"human": "user", "ai": "assistant"}  
for msg in msgs.messages:  
    st.chat_message(avatars[msg.type]).write(msg.content)  
  
if user_query := st.chat_input(placeholder="è¯·é—®ä»»ä½•å…³äºæ–‡æ¡£çš„é—®é¢˜!"):  
    # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    st.chat_message("user").write(user_query)  
    
    # å°†ç”¨æˆ·é—®é¢˜æ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­ï¼ˆåªæ·»åŠ ä¸€æ¬¡ï¼‰
    msgs.add_user_message(user_query)
  
    with st.chat_message("assistant"):  
        retrieval_container = st.container()
        response_placeholder = st.empty()
        stream_handler = StreamHandler(response_placeholder)  
        
        # åˆ›å»ºä¸€ä¸ªçŠ¶æ€æŒ‡ç¤ºå™¨
        with st.status("æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜...", expanded=True) as status:
            st.write("æ­£åœ¨æœç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”Ÿæˆå›ç­”...")
            
            # ä½¿ç”¨å¼‚æ­¥æ–¹å¼æ‰§è¡ŒAgentæŸ¥è¯¢
            async def process_query():
                # ä½¿ç”¨stream_handlerç«‹å³å°†ç”Ÿæˆçš„å†…å®¹æ˜¾ç¤ºåˆ°å‰ç«¯
                response = await agent_executor.ainvoke(
                    {"input": user_query},
                    {"callbacks": [stream_handler]}
                )
                
                # å®æ—¶æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆå¦‚æœæœ‰ï¼‰
                for step in response["intermediate_steps"]:
                    if step[0].tool == "pdf_search":
                        with retrieval_container:
                            st.write("**æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹:**")
                            st.markdown(step[1])
                
                status.update(label="å¤„ç†å®Œæˆ!", state="complete")
                
                # å°†AIå›ç­”æ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­
                if "output" in response:
                    # å®æ—¶æ›´æ–°å†å²ä¿¡æ¯
                    msgs.add_ai_message(response["output"])
                    # å¼ºåˆ¶åˆ·æ–°å‰ç«¯æ˜¾ç¤º
                    st.rerun()
                
                return response
            
            # åœ¨Streamlitä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°å¹¶ç«‹å³å±•ç¤ºç»“æœ
            response = asyncio.run(process_query())
            st.session_state.response = response
